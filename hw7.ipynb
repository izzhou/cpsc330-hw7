{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c790de75",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca5e1-717f-451f-b23f-ef411de8576f",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 7: Word embeddings and topic modeling \n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5cb55-0576-4596-ba0e-88d0ad7c39f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b799e3-3d4e-4151-9123-9d3733a63ac3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da966a1-1d24-42a8-b959-e42202e6b824",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "**Please be aware that this homework assignment requires installation of several packages in your course environment. It's possible that you'll encounter installation challenges, which might be frustrating. However, remember that solving these issues is not wasting time but it is an essential skill for anyone aspiring to work in data science or machine learning.**\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2024W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb. If the pdf or html also fail to render on Gradescope, please create two files for your homework: hw6a.ipynb with Exercise 1 and hw6b.ipynb with Exercises 2 and 3 and submit these two files in your submission.  \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 18, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings work well on a variety of text classification tasks. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads the word vectors trained on Wikipedia using an algorithm called Glove. You'll need `gensim` package in your cpsc330 conda environment to run the code below. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in this pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1002-2451-4008-aa83-54618c757bb3",
   "metadata": {},
   "source": [
    "Now that we have GloVe Wiki vectors loaded in `glove_wiki_vectors`, let's explore the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Come up with a list of 4 words of your choice and find similar words to these words using `glove_wiki_vectors` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe4d0-4206-4747-9638-7782cca51d3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d59251b0-12c0-41cf-867b-3e5cb90f4d8d",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'badminton':\n",
      "  volleyball: 0.8076\n",
      "  tennis: 0.7742\n",
      "  gymnastics: 0.7268\n",
      "  weightlifting: 0.7062\n",
      "  snooker: 0.7043\n",
      "  taekwondo: 0.6897\n",
      "  championships: 0.6888\n",
      "  judo: 0.6794\n",
      "  softball: 0.6760\n",
      "  netball: 0.6698\n"
     ]
    }
   ],
   "source": [
    "similar_words = glove_wiki_vectors.most_similar('badminton')\n",
    "print(\"Similar words to 'badminton':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a9bd11-2a14-44f5-8093-b0a696324ce1",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'baseball':\n",
      "  basketball: 0.7645\n",
      "  leagues: 0.7626\n",
      "  football: 0.7606\n",
      "  league: 0.7323\n",
      "  hockey: 0.7312\n",
      "  sports: 0.7067\n",
      "  nfl: 0.7048\n",
      "  soccer: 0.7028\n",
      "  mlb: 0.6976\n",
      "  yankees: 0.6953\n"
     ]
    }
   ],
   "source": [
    "similar_words = glove_wiki_vectors.most_similar('baseball')\n",
    "print(\"Similar words to 'baseball':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b922b29-e591-41a7-b82c-9bcc794d0f10",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'diving':\n",
      "  dive: 0.7657\n",
      "  swimming: 0.7169\n",
      "  scuba: 0.7153\n",
      "  snorkeling: 0.6957\n",
      "  diver: 0.6329\n",
      "  swim: 0.6257\n",
      "  snorkelling: 0.6025\n",
      "  divers: 0.5953\n",
      "  surfing: 0.5818\n",
      "  skiing: 0.5737\n"
     ]
    }
   ],
   "source": [
    "similar_words = glove_wiki_vectors.most_similar('diving')\n",
    "print(\"Similar words to 'diving':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1caa0fc7-900a-4ef5-817f-166cafc8cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'hockey':\n",
      "  basketball: 0.8042\n",
      "  football: 0.7834\n",
      "  nhl: 0.7604\n",
      "  soccer: 0.7441\n",
      "  baseball: 0.7312\n",
      "  league: 0.7092\n",
      "  skating: 0.6704\n",
      "  lacrosse: 0.6692\n",
      "  team: 0.6620\n",
      "  games: 0.6572\n"
     ]
    }
   ],
   "source": [
    "similar_words = glove_wiki_vectors.most_similar('hockey')\n",
    "print(\"Similar words to 'hockey':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24714da4-3ee5-424e-9a21-bd4b33de2e08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461c1d5-42ff-4267-bf04-e3642c5b40bb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of `glove_wiki_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b37d06f-81c7-4120-8dc7-2270105d5ecb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427682-97e3-4fa2-b2d6-84940fce9e27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7c8c6c4-7113-4cb2-98f8-d34ec1c632e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'coast' and 'shore': 0.7000\n",
      "Cosine similarity between 'clothes' and 'closet': 0.5463\n",
      "Cosine similarity between 'old' and 'new': 0.6432\n",
      "Cosine similarity between 'smart' and 'intelligent': 0.7553\n",
      "Cosine similarity between 'dog' and 'cat': 0.8798\n",
      "Cosine similarity between 'tree' and 'lawyer': 0.0767\n"
     ]
    }
   ],
   "source": [
    "for word1, word2 in word_pairs:\n",
    "    try:\n",
    "        similarity = glove_wiki_vectors.similarity(word1, word2)\n",
    "        print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: One of the words '{word1}' or '{word2}' not found in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a6420-ae43-4469-99e1-59a966238a43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9186e-ab44-43e5-8a96-7a1c4130b598",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Representation of all words in English\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. The `test_words` list below contains a few new words (called neologisms) and biomedical domain-specific abbreviations. Write code to check whether `glove_wiki_vectors` have representation for these words or not. \n",
    "> If a given word `word` is in the vocabulary, `word in glove_wiki_vectors` will return True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be6a6488-5139-43cb-a88c-b1d45df700cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    \"covididiot\",\n",
    "    \"fomo\",\n",
    "    \"frenemies\",\n",
    "    \"anthropause\",\n",
    "    \"photobomb\",\n",
    "    \"selfie\",\n",
    "    \"pxg\",  # Abbreviation for pseudoexfoliative glaucoma\n",
    "    \"pacg\",  # Abbreviation for primary angle closure glaucoma\n",
    "    \"cct\",  # Abbreviation for central corneal thickness\n",
    "    \"escc\",  # Abbreviation for esophageal squamous cell carcinoma\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfef3be-698f-4792-b7c3-46fac945e7f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44097c5d-97a0-450e-af88-73c9a4c90c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'covididiot' does not exist in the vocabulary\n",
      "'fomo' does not exist in the vocabulary\n",
      "'frenemies' exists in the vocabulary\n",
      "'anthropause' does not exist in the vocabulary\n",
      "'photobomb' does not exist in the vocabulary\n",
      "'selfie' does not exist in the vocabulary\n",
      "'pxg' does not exist in the vocabulary\n",
      "'pacg' does not exist in the vocabulary\n",
      "'cct' exists in the vocabulary\n",
      "'escc' exists in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    exists = word in glove_wiki_vectors\n",
    "    status = \"exists\" if exists else \"does not exist\"\n",
    "    print(f\"'{word}' {status} in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2155dbe-979f-4e4d-bd4c-04d2a5f0be20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea8f10-8bfb-4698-b76c-60f5f8256d5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Stereotypes and biases in embeddings\n",
    "rubric={points}\n",
    "\n",
    "Word vectors contain lots of useful information. But they also contain stereotypes and biases of the texts they were trained on. In the lecture, we saw an example of gender bias in Google News word embeddings. Here we are using pre-trained embeddings trained on Wikipedia data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore whether there are any worrisome biases or stereotypes present in these embeddings by trying out at least 4 examples. You can use the following two methods or other methods of your choice to explore this. \n",
    "    - the `analogy` function below which gives word analogies (an example shown below)\n",
    "    - [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods (an example is shown below)\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a90c0cd-7cf2-4f82-a617-9a87c526281d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaeec14-64c1-4226-b60f-849489077ddd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examples of using analogy to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e914a9fa-e1e9-4182-a82d-41737020e44f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.773523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.718943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.682433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.675068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.672603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>0.664246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.652045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.639333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.638750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0        nurse  0.773523\n",
       "1    physician  0.718943\n",
       "2      doctors  0.682433\n",
       "3      patient  0.675068\n",
       "4      dentist  0.672603\n",
       "5     pregnant  0.664246\n",
       "6      medical  0.652045\n",
       "7      nursing  0.645348\n",
       "8       mother  0.639333\n",
       "9     hospital  0.638750"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fceb0128-3c9b-4674-ae40-da3d80f3f00c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1428324"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"aboriginal\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "774cb08e-95bc-458b-bc78-ee3fcf2b2f7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.351824"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bef6d-9a57-41a4-8a83-a65d7bc07783",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a227771e-f8b7-4bad-9881-eb5528f10e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n",
      "Similarity between 'nurse' and 'female': 0.46290737\n",
      "Similarity between 'nurse' and 'male': 0.39995456\n"
     ]
    }
   ],
   "source": [
    "#Gender Bias in professions\n",
    "print(f\"Similarity between 'nurse' and 'female':\", glove_wiki_vectors.similarity(\"nurse\", \"female\"))\n",
    "print(f\"Similarity between 'nurse' and 'male':\", glove_wiki_vectors.similarity(\"nurse\", \"male\"))b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6025b7c9-5ffd-4076-8d44-0440d01efe69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities for 'asian':\n",
      "- with 'educated': 0.2095\n",
      "- with 'poverty': 0.4143\n",
      "- with 'crime': 0.2857\n",
      "- with 'success': 0.4309\n",
      "\n",
      "Similarities for 'african':\n",
      "- with 'educated': 0.3677\n",
      "- with 'poverty': 0.5122\n",
      "- with 'crime': 0.3481\n",
      "- with 'success': 0.3853\n",
      "\n",
      "Similarities for 'european':\n",
      "- with 'educated': 0.2519\n",
      "- with 'poverty': 0.2904\n",
      "- with 'crime': 0.3276\n",
      "- with 'success': 0.5152\n",
      "\n",
      "Similarities for 'hispanic':\n",
      "- with 'educated': 0.3824\n",
      "- with 'poverty': 0.3769\n",
      "- with 'crime': 0.3502\n",
      "- with 'success': 0.2897\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Cultural and ethnic stereotypes\n",
    "words_to_check = [\"educated\", \"poverty\", \"crime\", \"success\"]\n",
    "ethnicities = [\"asian\", \"african\", \"european\", \"hispanic\"]\n",
    "\n",
    "for ethnicity in ethnicities:\n",
    "    print(f\"\\nSimilarities for '{ethnicity}':\")\n",
    "    for word in words_to_check:\n",
    "        similarity = glove_wiki_vectors.similarity(ethnicity, word)\n",
    "        print(f\"- with '{word}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "deaa74e9-858e-4b28-a7a8-6b6b522c1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities for 'old':\n",
      "- with 'competent': 0.1696\n",
      "- with 'rich': 0.4015\n",
      "- with 'productive': 0.2191\n",
      "\n",
      "Similarities for 'young':\n",
      "- with 'competent': 0.3665\n",
      "- with 'rich': 0.5292\n",
      "- with 'productive': 0.3582\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Age-related bias\n",
    "age_terms = [\"old\", \"young\"]\n",
    "attributes = [\"competent\", \"rich\", \"productive\"]\n",
    "\n",
    "for age in age_terms:\n",
    "    print(f\"\\nSimilarities for '{age}':\")\n",
    "    for attr in attributes:\n",
    "        similarity = glove_wiki_vectors.similarity(age, attr)\n",
    "        print(f\"- with '{attr}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95e306e7-26f3-4367-8935-0be49b86be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarities for 'educated':\n",
      "- with 'successful': 0.3909\n",
      "- with 'intelligent': 0.4535\n",
      "- with 'wealthy': 0.5864\n",
      "- with 'criminal': 0.1732\n",
      "\n",
      "Similarities for 'uneducated':\n",
      "- with 'successful': 0.0247\n",
      "- with 'intelligent': 0.4124\n",
      "- with 'wealthy': 0.3658\n",
      "- with 'criminal': 0.0363\n",
      "\n",
      "Similarities for 'university':\n",
      "- with 'successful': 0.3606\n",
      "- with 'intelligent': 0.1985\n",
      "- with 'wealthy': 0.2369\n",
      "- with 'criminal': 0.3150\n",
      "\n",
      "Similarities for 'dropout':\n",
      "- with 'successful': 0.0670\n",
      "- with 'intelligent': 0.0237\n",
      "- with 'wealthy': 0.1315\n",
      "- with 'criminal': 0.0834\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Education-related bias\n",
    "edu_terms = [\"educated\", \"uneducated\", \"university\", \"dropout\"]\n",
    "attributes = [\"successful\", \"intelligent\", \"wealthy\", \"criminal\"]\n",
    "\n",
    "for term in edu_terms:\n",
    "    print(f\"\\nSimilarities for '{term}':\")\n",
    "    for attr in attributes:\n",
    "            similarity = glove_wiki_vectors.similarity(term, attr)\n",
    "            print(f\"- with '{attr}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f04b87-5fa0-4eb4-bb50-cb21aa7ffd1c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474b6ac-25f3-45f1-97db-bfadf71d9f80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Discuss your observations from 1.4. Are there any worrisome biases in these embeddings trained on Wikipedia?   \n",
    "2. Give an example of how using embeddings with biases could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95ea8-80c0-4184-8b89-1fa1581404f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c125c3",
   "metadata": {},
   "source": [
    "1. \n",
    "1)When researching on gender stereotype on professions, there is higher similarity between 'nurse' and 'female' compares to 'male', suggesting a stereotype to assume a nurse as a woman.\n",
    "\n",
    "2)The Cultural and ethnic stereotypes analysis shows concerning ethnic biases. For example, \"european\" has the strongest association with \"success\" (0.5152) while having the lowest association with \"poverty\" (0.2904), whereas \"african\" shows the opposite pattern with the highest association with \"poverty\" (0.5122) and lower association with \"success\" (0.3853), reflecting harmful stereotypes present in the training data.\n",
    "\n",
    "3)The Age-related analysis shows clear age-based bias, with \"young\" having consistently higher similarity scores for positive attributes like \"competent\" (0.3665), \"rich\" (0.5292), and \"productive\" (0.3582) compared to \"old\", which shows notably lower associations with these same qualities, particularly for competence (0.1696) and productivity (0.2191).\n",
    "\n",
    "4)The education-related analysis shows strong educational bias, where being \"educated\" has high positive correlations with \"wealthy\" (0.5864) and \"intelligent\" (0.4535), while \"dropout\" has very low associations with these same positive attributes and the lowest scores across all positive traits. This bias is particularly stark in terms of success, where \"educated\" scores 0.3909 for \"successful\" while \"dropout\" scores only 0.0670, reflecting societal prejudices about educational status.\n",
    "\n",
    "2. \n",
    "An automated hiring system using these biased word embeddings could severely discriminate against job candidates in multiple ways. The system would unfairly prefer young, white, educated male candidates while putting others at a disadvantage - like giving lower scores to older workers, people of colors, women, or those without college degrees. When someone faces several of these biases at once - like an older woman of color without a college degree - the negative impact would be even worse, making it much harder for them to get hired despite their actual skills and experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb13f7-e08c-4f8d-86c9-b1602cc8a5b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Topic modeling \n",
    "\n",
    "The goal of topic modeling is discovering high-level themes in a large collection of texts. \n",
    "\n",
    "In this homework, you will explore topics in [the 20 newsgroups text dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) using `scikit-learn`'s `LatentDirichletAllocation` (LDA) model. \n",
    "\n",
    "Usually, topic modeling is used for discovering abstract \"topics\" that occur in a collection of documents when you do not know the actual topics present in the documents. But 20 newsgroups text dataset is labeled with categories (e.g., sports, hardware, religion), and you will be able to cross-check the topics discovered by your model with these available topics. \n",
    "\n",
    "The starter code below loads the train and test portion of the data and convert the train portion into a pandas DataFrame. For speed, we will only consider documents with the following 8 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "327ca91e-cf57-4481-b4cc-46c29a9849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9afa3b30-7f32-48ec-8291-69c964a38c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name  \n",
       "0        talk.politics.guns  \n",
       "1             comp.graphics  \n",
       "2             comp.graphics  \n",
       "3            comp.windows.x  \n",
       "4     talk.politics.mideast  \n",
       "...                     ...  \n",
       "4558          comp.graphics  \n",
       "4559         comp.windows.x  \n",
       "4560     talk.politics.guns  \n",
       "4561         comp.windows.x  \n",
       "4562  talk.politics.mideast  \n",
       "\n",
       "[4563 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"rec.sport.hockey\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.windows.x\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.guns\",\n",
    "]  # We'll only consider these categories out of 20 categories for speed.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=cats\n",
    ")\n",
    "X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target\n",
    "df = pd.DataFrame(X_news_train, columns=[\"text\"])\n",
    "df[\"target\"] = y_news_train\n",
    "df[\"target_name\"] = [\n",
    "    newsgroups_train.target_names[target] for target in newsgroups_train.target\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8095c853-d4d5-49a3-bda0-129ffd2f690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cdcfc-7545-4a40-a9b3-34b58bb38365",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21b066-8de6-42a7-8e4e-097fd8727367",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Preprocessing using [spaCy](https://spacy.io/)\n",
    "rubric={points}\n",
    "\n",
    "Preprocessing is a crucial step before carrying out topic modeling and it markedly affects topic modeling results. In this exercise, you'll prepare the data using [spaCy](https://spacy.io/) for topic modeling. \n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "- Write code using [spaCy](https://spacy.io/) to preprocess the `text` column in the given dataframe `df` and save the processed text in a new column called `text_pp` within the same dataframe.\n",
    "\n",
    "If you do not have spaCy in your course environment, you'll have to [install it](https://spacy.io/usage) and download the pretrained model en_core_web_md. \n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "\n",
    "Note that there is no such thing as \"perfect\" preprocessing. You'll have to make your own judgments and decisions on which tokens are likely to be more informative for the given task. Some common text preprocessing steps for topic modeling include: \n",
    "- getting rid of slashes, new-line characters, or any other non-informative characters\n",
    "- sentence segmentation and tokenization      \n",
    "- replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "- getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "- excluding stopwords and punctuation \n",
    "- lemmatization\n",
    "\n",
    "\n",
    "> Check out [these available attributes](https://spacy.io/api/token#attributes) for `token` in spaCy which might help you with preprocessing. \n",
    "\n",
    "> You can also get rid of words with specific POS tags. [Here](https://universaldependencies.org/u/pos/) is the list of part-of-speech tags used in spaCy. \n",
    "\n",
    "> You may have to use regex to clean text before passing it to spaCy. Also, you might have to go back and forth between preprocessing in this exercise and and topic modeling in Exercise 2 before finalizing preprocessing steps. \n",
    "\n",
    "> Note that preprocessing the corpus might take some time. So here are a couple of suggestions: 1) During the debugging phase, work on a smaller subset of the data. 2) Once you finalize the preprocessing part, you might want to save the preprocessed data in a CSV and work with this CSV so that you don't run the preprocessing part every time you run the notebook. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6816e7a3-d6d4-4bef-81f4-9b4fdf638175",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dbe6-2353-4c5a-80bf-2b884a170a29",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8be7c802-d10b-4929-9664-34274299e884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Remove unwanted characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  \n",
    "    text = re.sub(r\"\\n\", \" \", text)    \n",
    "    text = re.sub(r\"http\\S+\", \"URL\", text)  \n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"NUM\", text)\n",
    "    \n",
    "    # Step 2: Process the text with spaCy (tokenization and lemmatization)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and not token.like_email:\n",
    "            if token.pos_ not in [\"PRON\", \"DET\", \"PART\", \"SYM\", \"SPACE\"]:  \n",
    "                lemma = token.lemma_.lower().strip() \n",
    "                tokens.append(lemma)\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"text_pp\"] = df[\"text\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba75d96d-01dc-4b3b-b222-67c63b5fd0c5",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  You know, I was reading 18 U.S.C. 922 and some...   \n",
      "1  \\n\\n\\nIt's not a bad question: I don't have an...   \n",
      "2  \\nActuallay I don't, but on the other hand I d...   \n",
      "3  The following problem is really bugging me,\\na...   \n",
      "4  \\n\\n  This is the latest from UPI \\n\\n     For...   \n",
      "\n",
      "                                             text_pp  \n",
      "0  know read num u.s.c. num sence wonder help u.s...  \n",
      "1  bad question ref list algorithm think bit hard...  \n",
      "2  actuallay hand support idea have newsgroup asp...  \n",
      "3  follow problem bug appreciate help create wind...  \n",
      "4  late upi foreign ministry spokesman ferhat ata...  \n"
     ]
    }
   ],
   "source": [
    "print(df[[\"text\", \"text_pp\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0782a094-bd30-4e80-9678-8f1197ca6a28",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"preprocessed_20newsgroups.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c2b633b-6601-4b83-8668-800a99d3ba20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay hand support idea have newsgroup asp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>follow problem bug appreciate help create wind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi,\\n   I'd like to subscribe to Leadership Ma...</td>\n",
       "      <td>5</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>hi like subscribe leadership magazine wonder d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "2  \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3  The following problem is really bugging me,\\na...       2   \n",
       "4  \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "5  Hi,\\n   I'd like to subscribe to Leadership Ma...       5   \n",
       "\n",
       "              target_name                                            text_pp  \n",
       "2           comp.graphics  actuallay hand support idea have newsgroup asp...  \n",
       "3          comp.windows.x  follow problem bug appreciate help create wind...  \n",
       "4   talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  \n",
       "5  soc.religion.christian  hi like subscribe leadership magazine wonder d...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbad26-84c1-41ed-8201-0843924b3166",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f9bd5-31ef-43dd-9ec7-e0e1cd9a9714",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Justification\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Outline the preprocessing steps you carried out in the previous exercise (bullet point format is fine), providing a brief justification when necessary. \n",
    "\n",
    "> You might want to wait to answer this question till you are done with Exercise 2 and you have finalized the preprocessing steps in 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8df885-0fb7-4ec7-bef6-bb0c3cd82e9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a06bf7",
   "metadata": {},
   "source": [
    "1. **Removal of Extra Whitespace and Newline Characters**:\n",
    "   - Used regular expressions to remove unnecessary whitespace (`\\s+`) and newline characters (`\\n`). This cleans the text and standardizes the format for further processing.\n",
    "\n",
    "2. **Replacement of URLs and Numbers with Generic Tokens**:\n",
    "   - Replaced URLs with \"URL\" and numbers with \"NUM\" to remove any specific, irrelevant content that may not contribute meaningfully to the topics.\n",
    "\n",
    "3. **Tokenization and Lemmatization**:\n",
    "   - Utilized spaCy to split the text into individual tokens and lemmatize them to reduce words to their base form. Lemmatization helps in grouping similar words and reducing redundancy in the vocabulary.\n",
    "\n",
    "4. **Exclusion of Stop Words, Punctuation, and Non-informative Tokens**:\n",
    "   - Removed stop words (e.g., \"the,\" \"is\") and punctuation to focus on the meaningful content.\n",
    "   - Also excluded tokens with specific Part-of-Speech (POS) tags, such as pronouns, determiners, particles, symbols, and spaces. This further reduces irrelevant information, focusing on nouns, verbs, and adjectives that are likely to convey the main themes.\n",
    "\n",
    "5. **Conversion to Lowercase**:\n",
    "   - Converted all tokens to lowercase to avoid case-sensitive differences in words, ensuring consistency across the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22f0e-2526-4f83-94ee-a625c7a5ed04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080f91c-6801-4b89-9fbc-1b574b09915c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Build a topic model using sklearn's LatentDirichletAllocation\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build LDA models on the preprocessed data using using [sklearn's `LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and random state 42. Experiment with a few values for the number of topics (`n_components`). Pick a reasonable number for the number of topics and briefly justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e472711-3c92-4b82-a31d-5d4bf55dbf2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5a649",
   "metadata": {},
   "source": [
    "We selected 8 topics because it provides distinct, interpretable themes that align well with the categories in the dataset (e.g., sports, religion, politics, technology) without excessive overlap or over-specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88908be8-85df-40bf-9d6d-0b48948a7137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics for different n_components:\n",
      "\n",
      "Number of topics: 5\n",
      "Topic #1: num edu period new pub jpeg play la vs game\n",
      "Topic #2: god people think know believe jesus num say thing time\n",
      "Topic #3: file program use window image num entry widget server display\n",
      "Topic #4: num gun people state israel right law jews weapon government\n",
      "Topic #5: num year say go game think people know come team\n",
      "\n",
      "Number of topics: 8\n",
      "Topic #1: num period la new pts vs edu pp st play\n",
      "Topic #2: god jesus num know find book church question bible time\n",
      "Topic #3: num file window program use entry widget server set line\n",
      "Topic #4: num gun law file firearm weapon state control crime bill\n",
      "Topic #5: game year team num go say think good know play\n",
      "Topic #6: num armenian israel turkish jews people armenians israeli government jewish\n",
      "Topic #7: image file format color graphic jpeg software available ftp package\n",
      "Topic #8: people think god know believe say like thing right life\n",
      "\n",
      "Number of topics: 10\n",
      "Topic #1: num period la pts vs new play pp st power\n",
      "Topic #2: window num use file server widget run available program work\n",
      "Topic #3: num entry file output program line char int section rule\n",
      "Topic #4: gun num weapon firearm law control crime state file fire\n",
      "Topic #5: game year team go say think good know come play\n",
      "Topic #6: num israel israeli arab university jews jewish gm research state\n",
      "Topic #7: image file format jpeg color gif graphic program bit convert\n",
      "Topic #8: people think know right question like exist atheist argument believe\n",
      "Topic #9: god jesus know think believe people say church time thing\n",
      "Topic #10: num armenian turkish armenians people jews government turkey greek armenia\n",
      "\n",
      "Number of topics: 15\n",
      "Topic #1: num period la pts vs play pp st power new\n",
      "Topic #2: god exist question atheist think believe know argument religion belief\n",
      "Topic #3: thank window know appreciate advance hi like point find problem\n",
      "Topic #4: people fbi num koresh fire president compound know question gas\n",
      "Topic #5: game year team good play player think get season time\n",
      "Topic #6: israel num israeli entry arab state people jews right war\n",
      "Topic #7: jpeg image gif color father view spirit son quality viewer\n",
      "Topic #8: say people know go come num tell think like kill\n",
      "Topic #9: god jesus people think know say believe thing christ time\n",
      "Topic #10: law jews num church people greek turkish right book state\n",
      "Topic #11: num armenian turkish armenians people genocide armenia turks child russian\n",
      "Topic #12: marriage jewish marry couple ceremony gl texas married wife mark\n",
      "Topic #13: num gun weapon firearm law file crime control criminal handgun\n",
      "Topic #14: num file program use image available window include server run\n",
      "Topic #15: hockey team nhl think like new player league let cup\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert preprocessed text data to a document-term matrix\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2) \n",
    "dtm = vectorizer.fit_transform(df[\"text_pp\"])\n",
    "\n",
    "# Experiment with different values for the number of topics\n",
    "n_topics_options = [5, 8, 10, 15] \n",
    "lda_models = {}\n",
    "for n_topics in n_topics_options:\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "    lda_models[n_topics] = lda\n",
    "\n",
    "# Display the top words for each topic for different values of n_components\n",
    "def display_topics(model, feature_names, num_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
    "        print(f\"Topic #{topic_idx + 1}: {' '.join(top_words)}\")\n",
    "\n",
    "print(\"LDA topics for different n_components:\")\n",
    "for n_topics, lda_model in lda_models.items():\n",
    "    print(f\"\\nNumber of topics: {n_topics}\")\n",
    "    display_topics(lda_model, vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1afbdb-b419-4d17-a7fa-0dfd9e618f15",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ff43-188c-4d9a-8404-691cd563dd9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 Exploring word topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For the number of topics you picked in the previous exercise, show top 10 words for each of your topics and suggest labels for each of the topics (similar to how we came up with labels \"health and nutrition\", \"fashion\", and \"machine learning\" in the toy example we saw in class). \n",
    "\n",
    "> If your topics do not make much sense, you might have to go back to preprocessing in Exercise 2.1, improve it, and train your LDA model again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe9de-c9d9-41a1-bf5a-10220ce6fa38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "967136f3-5746-4568-8d2d-d48c4d3be36c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: armenian jews people turkish num armenians say year genocide government\n",
      "Topic #2: israel israeli right num state people greek arab attack country\n",
      "Topic #3: num file program image use window available include entry server\n",
      "Topic #4: num period la play vs pts pp power goal new\n",
      "Topic #5: num say god people know come jesus think time tell\n",
      "Topic #6: year point think islam career morris kk read big day\n",
      "Topic #7: gun god people think believe know question law thing claim\n",
      "Topic #8: game team good player think play year like know time\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(df['text_pp'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "def display_topics(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(f\"Topic #{topic_idx+1}: {' '.join(top_words)}\")\n",
    "    return topics\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topics = display_topics(lda, feature_names, 10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce0ecd3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Topic #1 Events in Armenian History\n",
    "**Why**: These words talk about what happened to Armenian people in the past\n",
    "\n",
    "## Topic #2 Middle East News\n",
    "**Why**: Words are about events and conflicts in Israel and nearby countries\n",
    "\n",
    "## Topic #3 Computer Programs\n",
    "**Why**: Words are about using and making computer software\n",
    "\n",
    "## Topic #4 Game Scores\n",
    "**Why**: Words are about points and goals in sports games\n",
    "\n",
    "## Topic #5 God and Faith\n",
    "**Why**: Words are about religious beliefs and Jesus\n",
    "\n",
    "## Topic #6 Islam and Life\n",
    "**Why**: Words are about Islam and everyday experiences\n",
    "\n",
    "## Topic #7 Gun Rules \n",
    "**Why**: Words are about guns and rules about them\n",
    "\n",
    "## Topic #8 Sports Teams\n",
    "**Why**: Words are about how teams and players do in sports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cbbb9-1057-43ef-ac63-842e3e4cb43c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53117b-897c-4eba-b1bd-dfedcb3d35ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Exploring document topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show the document topic assignment of the first five documents from `df`. \n",
    "2. Comment on the document topic assignment of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12934a67-c48d-4c42-a856-248615c6202b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f09783a-5644-46ac-bb2f-d60947a13d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Topic 7: 0.823\n",
      "Topic 4: 0.104\n",
      "Topic 2: 0.062\n",
      "Dominant Topic: 7\n",
      "\n",
      "Document 2:\n",
      "Topic 6: 0.931\n",
      "Topic 4: 0.059\n",
      "Dominant Topic: 6\n",
      "\n",
      "Document 3:\n",
      "Topic 7: 0.482\n",
      "Topic 3: 0.384\n",
      "Topic 5: 0.123\n",
      "Dominant Topic: 7\n",
      "\n",
      "Document 4:\n",
      "Topic 8: 0.506\n",
      "Topic 5: 0.473\n",
      "Dominant Topic: 8\n",
      "\n",
      "Document 5:\n",
      "Topic 1: 0.639\n",
      "Topic 2: 0.284\n",
      "Topic 6: 0.062\n",
      "Dominant Topic: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get document-topic distributions\n",
    "doc_topics = lda.transform(dtm)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    # Get probabilities higher than 1%\n",
    "    topic_probs = [(j+1, prob) for j, prob in enumerate(doc_topics[i]) if prob > 0.01]\n",
    "    # Sort by probability in descending order\n",
    "    topic_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Display topic probabilities\n",
    "    for topic_num, prob in topic_probs:\n",
    "        print(f\"Topic {topic_num}: {prob:.3f}\")\n",
    "    print(f\"Dominant Topic: {doc_topics[i].argmax() + 1}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f3bd9",
   "metadata": {},
   "source": [
    "Document 1:\n",
    "This document is strongly focused on Gun Rules (Topic 7) with 82.3% probability, with a minor connection to Game Scores (Topic 4). Since Gun Rules is so dominant, this document is likely a discussion about gun legislation or gun control policies.\n",
    "\n",
    "Document 2:\n",
    "This document shows an extremely strong alignment with Islam and Life (Topic 6) at 93.1%. The high probability for a single topic suggests this is a highly focused discussion about Islamic practices or daily life in relation to Islam.\n",
    "\n",
    "Document 3:\n",
    "This document shows a mixed distribution between Gun Rules (Topic 7, 48.2%) and Computer Programs (Topic 3, 38.4%), with some religious content (God and Faith, Topic 5, 12.3%). This combination might suggest a discussion that bridges multiple topics, perhaps about technology in relation to gun rights or regulations.\n",
    "\n",
    "Document 4:\n",
    "This document is almost evenly split between Sports Teams (Topic 8, 50.6%) and God and Faith (Topic 5, 47.3%). This is an interesting combination, which might indicate a discussion about religion in sports or perhaps a sports team with religious beliefs.\n",
    "\n",
    "Document 5:\n",
    "This document is primarily about Armenian History (Topic 1, 63.9%) with significant coverage of Middle East News (Topic 2, 28.4%). This combination makes sense as these topics are geographically related, suggesting a discussion about Armenian history in the context of broader Middle Eastern events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e21cac-5f1a-4480-b683-47c3b41a1199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42685c5-01cb-4495-aae6-f803d6f75172",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Short answer questions \n",
    "<hr>\n",
    "\n",
    "rubric={points}\n",
    "\n",
    "1. Briefly explain how content-based filtering works in the context of recommender systems. \n",
    "2. Discuss at least two negative consequences of recommender systems.\n",
    "3. What is transfer learning in natural language processing? Briefly explain.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01807641-531e-454a-9374-fc8ad04bc30b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f9fa2",
   "metadata": {},
   "source": [
    "1. The content-based filtering works by looking at what a user likes and finding similar items for them. For example, if you watch love movies with Leonardo DiCaprio, it will recommend other love movies or more Leonardo DiCaprio movies. \n",
    "\n",
    "2. \n",
    "- Filter Bubbles: People get stuck seeing only things they already agree with or like, making them less open to new ideas or different viewpoints.\n",
    "- Privacy Issues: These systems collect lots of personal data about what we like, watch, and buy, which can be misused or leaked.\n",
    "\n",
    "3. Transfer learning is a machine learning technique in which knowledge gained through one task or dataset is used to improve model performance on another related task and/or different dataset. It's like teaching someone a new language when they already know a similar one. The system first learns basic language skills from a large amount of text (like learning English), and then uses that knowledge to do specific tasks better (like understanding movie reviews). It's more efficient because it doesn't have to learn everything from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929238-7f2c-421e-bb9a-32f0debf0636",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d6d11-0ef5-4315-9854-f64a427e62af",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491f1d3-dd84-4ed2-b2b3-9148ac8df7a5",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330] *",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
